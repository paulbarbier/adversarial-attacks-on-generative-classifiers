{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manal\\Desktop\\3A\\PGM\\adversarial-attacks-on-generative-classifiers\\.venv\\lib\\site-packages\\orbax\\checkpoint\\type_handlers.py:1346: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "import os\n",
    "from ml_collections import ConfigDict\n",
    "from pathlib import Path\n",
    "from utils import prepare_test_dataset\n",
    "from dataset_utils import get_dataset\n",
    "from jax import random\n",
    "from models.utils import sample_gaussian\n",
    "\n",
    "import models.ClassifierGFZ as ClassifierGFZ\n",
    "import models.ClassifierDFZ as ClassifierDFZ\n",
    "\n",
    "checkpoint_path = \"dfz-2-epochs-first-try-1\"\n",
    "path = os.path.join(Path.cwd(), Path(f\"checkpoints\"), Path(checkpoint_path))\n",
    "checkpoint = ocp.PyTreeCheckpointer().restore(path, item=None)\n",
    "\n",
    "config = ConfigDict(checkpoint[\"config\"])\n",
    "dataset_config = ConfigDict(checkpoint[\"dataset_config\"])\n",
    "\n",
    "if config.model_name == \"GFZ\":\n",
    "    classifier = ClassifierGFZ\n",
    "elif config.model_name == \"DFZ\":\n",
    "    classifier = ClassifierDFZ\n",
    "else:\n",
    "    raise NotImplementedError(config.model_name)\n",
    "\n",
    "_, test_ds = get_dataset(config.dataset)\n",
    "test_images, test_labels = prepare_test_dataset(\n",
    "    test_ds, dataset_config\n",
    "    )\n",
    "\n",
    "trained_params = checkpoint[\"params\"]\n",
    "\n",
    "log_likelyhood_fn = classifier.log_likelyhood_A\n",
    "\n",
    "test_key = random.PRNGKey(config.seed)\n",
    "\n",
    "test_key, model, _ = classifier.create_and_init(\n",
    "    test_key, config, dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax import jacrev\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def init_data(test_key, n_samples=10):\n",
    "    idx = np.random.choice(range(len(test_images)), n_samples, replace=False)\n",
    "\n",
    "    all_xs = test_images[idx]\n",
    "    true_ys = test_labels[idx]\n",
    "    true_labels = np.argmax(true_ys, axis=1)\n",
    "\n",
    "    K = model.K\n",
    "    batch_size = n_samples\n",
    "    test_key, epsilons = sample_gaussian(test_key, (batch_size, model.n_classes * K, model.d_latent))\n",
    "    epsilons = epsilons[:n_samples*model.n_classes]\n",
    "    all_ys = nn.one_hot(jnp.repeat(jnp.arange(model.n_classes), K), model.n_classes, dtype=jnp.float32)\n",
    "    \n",
    "    return all_xs, true_labels, epsilons, all_ys, K, test_key\n",
    "\n",
    "def get_model_output(x, epsilon, y, K):\n",
    "    z, logit_q_z_xy, logit_p_x_z, logit_p_y_xz = jax.vmap(\n",
    "            partial(model.apply, {'params': trained_params}, train=False),\n",
    "            in_axes=(None, 0, 0)\n",
    "        )(x, y, epsilon)\n",
    "\n",
    "    ll = log_likelyhood_fn(\n",
    "            z, logit_q_z_xy, logit_p_x_z, logit_p_y_xz\n",
    "        ).reshape(model.n_classes, K)\n",
    "    ll = logsumexp(ll, axis=1) - np.log(K)\n",
    "    return ll\n",
    "\n",
    "def get_model_jacobian(x, epsilon, y, K):\n",
    "    return jacrev(get_model_output, argnums=0)(x, epsilon, y, K)\n",
    "\n",
    "def map_label_to_name(y):\n",
    "    labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "              \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "    return labels[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constrained_BFGS_B():\n",
    "    def __init__(self, f, grad, x0, bounds, maxiter=1000, eps=1e-8):\n",
    "        self.f = f\n",
    "        self.grad = grad\n",
    "        self.x0 = x0\n",
    "        self.bounds = bounds\n",
    "        self.maxiter = maxiter\n",
    "        self.eps = eps\n",
    "\n",
    "    def line_search(self, x, direction, alpha = 0.4, beta=0.8, max_iter=1000):\n",
    "        step_size = 1\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            if self.f(x + step_size  * direction) <= self.f(x) + step_size * alpha * direction.dot(self.grad(x)):\n",
    "                break\n",
    "            step_size  *= beta\n",
    "            i += 1\n",
    "            \n",
    "        return step_size\n",
    "    \n",
    "    def determine_active_set(self, x, bounds):\n",
    "        active_set = np.ones_like(x, dtype=bool)\n",
    "\n",
    "        for i, (lower_bound, upper_bound) in enumerate(bounds):\n",
    "            if lower_bound is not None and x[i] <= lower_bound:\n",
    "                active_set[i] = False  # Variable is at the lower bound\n",
    "            elif upper_bound is not None and x[i] >= upper_bound:\n",
    "                active_set[i] = False  # Variable is at the upper bound\n",
    "\n",
    "        return active_set\n",
    "    \n",
    "    def update_inverse_hessian_bfgs_b(self, Bk, sk, yk, active_set):\n",
    "        sk_active = sk[active_set]\n",
    "        yk_active = yk[active_set]\n",
    "\n",
    "        if not any(active_set):\n",
    "            return Bk\n",
    "\n",
    "        rho = 1 / np.dot(yk_active, sk_active)\n",
    "        term1 = np.eye(len(sk_active)) - np.outer(sk_active, yk_active) * rho\n",
    "        term2 = np.eye(len(sk_active)) - np.outer(yk_active, sk_active) * rho\n",
    "        Bk1_active = np.dot(term1, np.dot(Bk, term2)) + np.outer(rho * sk_active, sk_active)\n",
    "        Bk1 = Bk.copy()\n",
    "        Bk1[np.ix_(active_set, active_set)] = Bk1_active\n",
    "\n",
    "        return Bk1\n",
    "\n",
    "    def optimize(self):\n",
    "        x = self.x0\n",
    "        B = np.eye(len(x))\n",
    "        for i in range(self.maxiter):\n",
    "            g = self.grad(x)\n",
    "            if np.linalg.norm(g) < self.eps:\n",
    "                break\n",
    "            direction = -B.dot(g)\n",
    "            step_size = self.line_search(x, direction)\n",
    "            x_new = x + step_size * direction\n",
    "            s = x_new - x\n",
    "            y = self.grad(x_new) - g\n",
    "            active_set = self.determine_active_set(x_new, self.bounds)\n",
    "            B = self.update_inverse_hessian_bfgs_b(B, s, y, active_set)\n",
    "            x = x_new\n",
    "            print(self.f(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class L_BGFS_Attack():\n",
    "    def __init__(self, model, max_iter=100, learning_rate=1, p=2):\n",
    "        self.model = model\n",
    "        self.n_classes = model.n_classes\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        assert p > 1 \n",
    "        self.p = p\n",
    "        if self.p == np.inf:\n",
    "            self.q = 1\n",
    "        else:\n",
    "            self.q = self.p / (self.p - 1)\n",
    "\n",
    "    def qnorm(self, x):\n",
    "        return jnp.linalg.norm(x.flatten(), self.q)\n",
    "\n",
    "    def get_label(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jnp.argmax(val)\n",
    "\n",
    "    def get_likelihoods(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return val\n",
    "\n",
    "    def get_gradients(self, x):\n",
    "        J = get_model_jacobian(x, self.epsilon, self.y, self.K)\n",
    "        return J.flatten()\n",
    "\n",
    "    def loss(self, val, label):\n",
    "        label_one_hot_encoding = jax.nn.one_hot(jnp.array([label]), self.n_classes)\n",
    "        return optax.softmax_cross_entropy(val, label_one_hot_encoding)\n",
    "    \n",
    "    def project_to_bounds(self, x, bounds):\n",
    "        bounds_min = jnp.zeros_like(x)\n",
    "        bounds_max = jnp.ones_like(x)\n",
    "        return jnp.clip(x, bounds_min, bounds_max)\n",
    "\n",
    "    def get_perturbation(self, x, epsilon, all_ys, K):\n",
    "        corrupted_x = x.copy()\n",
    "        self.y = all_ys\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "        true_label = self.get_label(corrupted_x)\n",
    "        bounds = np.array([[0, 1] * len(corrupted_x.flatten())])\n",
    "        r = np.ones_like(corrupted_x)\n",
    "        max_perturbation_norm = jnp.linalg.norm(jax.device_put(r))\n",
    "        best_label = true_label\n",
    "        best_corrupted_x = corrupted_x\n",
    "\n",
    "        # Line search for c\n",
    "        for label in range(self.n_classes):\n",
    "            if label != true_label:\n",
    "                def get_problem(r):\n",
    "                    corrupted_x = self.project_to_bounds(x + r.reshape(x.shape), bounds)\n",
    "                    val = self.get_likelihoods(corrupted_x)\n",
    "                    return jnp.sum(self.qnorm(r) + self.loss(val, label))\n",
    "\n",
    "                i = 0\n",
    "                optimizer = optax.adam(learning_rate=self.learning_rate)\n",
    "                state = optimizer.init(jax.device_put(r.flatten()))\n",
    "                while i < self.max_iter:\n",
    "                    grad = jax.grad(get_problem)(jax.device_put(r.flatten()))\n",
    "                    updates, state = optimizer.update(grad, state)\n",
    "                    r = optax.apply_updates(jax.device_put(r.flatten()), updates).reshape(corrupted_x.shape)\n",
    "                    corrupted_x = self.project_to_bounds(corrupted_x + r, bounds)\n",
    "                    if self.get_label(corrupted_x) == label:\n",
    "                        break\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "                new_label = self.get_label(corrupted_x)\n",
    "                if new_label != label:\n",
    "                    print(\"Warning: did not find a perturbation\")\n",
    "                    perturbation_norm = -1\n",
    "                else:\n",
    "                    perturbation_norm = jnp.linalg.norm(jax.device_put(r))\n",
    "\n",
    "                # Choose minimal perturbation\n",
    "                if perturbation_norm < max_perturbation_norm:\n",
    "                    max_perturbation_norm = perturbation_norm\n",
    "                    best_label = new_label\n",
    "                    best_corrupted_x = corrupted_x\n",
    "\n",
    "        return best_corrupted_x, best_label, max_perturbation_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_performance(corruption_model, all_xs, epsilons, all_ys, K):\n",
    "    perturbation_norms = []\n",
    "    n_samples = len(all_xs)\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        x = all_xs[i]\n",
    "        epsilon = epsilons[i]\n",
    "        _, _, perturbation_norm = corruption_model.get_perturbation(x, epsilon, all_ys, K)\n",
    "        perturbation_norms.append(perturbation_norm)\n",
    "    return np.array(perturbation_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0.000576]\n",
      "[3.31776e-07]\n",
      "[1.17549435e-38]\n",
      "[1.17549435e-38]\n"
     ]
    }
   ],
   "source": [
    "#Testing optimization algorithm\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def grad(x):\n",
    "    return 2*x\n",
    "f = f\n",
    "g = grad\n",
    "bounds = [(0, 1)] * 1\n",
    "i = 0\n",
    "corrupted_x = np.array([1])\n",
    "print(f(corrupted_x))\n",
    "BFGS = Constrained_BFGS_B(f, g, corrupted_x, bounds)\n",
    "r = BFGS.optimize()\n",
    "print(f(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:39<00:00, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perturbation norm of L_BGFS Attack model (on 10 successful samples): 17.2475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "all_xs, true_labels, epsilons, all_ys, K, test_key = init_data(test_key, n_samples=n_samples)\n",
    "\n",
    "corruption_model = L_BGFS_Attack(model)\n",
    "\n",
    "perturbation_norms_BFGS = get_average_performance(corruption_model, all_xs, epsilons, all_ys, K)\n",
    "perturbation_norms_successful_BFGS = perturbation_norms_BFGS[perturbation_norms_BFGS != -1]\n",
    "n_successful_BFGS = len(perturbation_norms_successful_BFGS)\n",
    "n_successful_BFGS\n",
    "print(f'Average perturbation norm of L_BGFS Attack model (on {} successful samples): {np.mean(perturbation_norms_successful_BFGS):>.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wagner and carlini attack\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class WG_Attack():\n",
    "    def __init__(self, model, max_iter=100, learning_rate=1, p=2):\n",
    "        self.model = model\n",
    "        self.n_classes = model.n_classes\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        assert p > 1 \n",
    "        self.p = p\n",
    "        if self.p == np.inf:\n",
    "            self.q = 1\n",
    "        else:\n",
    "            self.q = self.p / (self.p - 1)\n",
    "\n",
    "    def qnorm(self, x):\n",
    "        return jnp.linalg.norm(x.flatten(), self.q)\n",
    "\n",
    "    def get_label(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jnp.argmax(val)\n",
    "\n",
    "    def get_likelihoods(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return val\n",
    "\n",
    "    def get_gradients(self, x):\n",
    "        J = get_model_jacobian(x, self.epsilon, self.y, self.K)\n",
    "        return J.flatten()\n",
    "\n",
    "    def loss(self, val, label):\n",
    "        label_one_hot_encoding = jax.nn.one_hot(jnp.array([label]), self.n_classes)\n",
    "        return optax.softmax_cross_entropy(val, label_one_hot_encoding)\n",
    "    \n",
    "    def f(self, x, target_label, k = 0):\n",
    "        val = self.get_likelihoods(x)\n",
    "        max_logit = jnp.max(val[jnp.arange(self.n_classes) != target_label])\n",
    "        logit_diff = jnp.maximum(max_logit - val[target_label], - k)\n",
    "        return logit_diff\n",
    "    \n",
    "    def get_objective(self, w, x, target_label, c, k = 0, type = \"L2\"):\n",
    "        if type == \"L2\":\n",
    "            norm = self.qnorm(1/2 * jnp.tanh(w) + 1/2 - x)\n",
    "        penalty = c * self.f(1/2 * jnp.tanh(w) + 1/2, target_label, k = k)\n",
    "        return norm + penalty\n",
    "    \n",
    "    def project_to_bounds(self, x):\n",
    "        bounds_min = jnp.zeros_like(x)\n",
    "        bounds_max = jnp.ones_like(x)\n",
    "        return jnp.clip(x, bounds_min, bounds_max)\n",
    "\n",
    "    def get_perturbation(self, x, epsilon, all_ys, K):\n",
    "        corrupted_x = x.copy()\n",
    "        self.y = all_ys\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "        true_label = self.get_label(corrupted_x)\n",
    "        w = np.ones_like(corrupted_x)\n",
    "        max_perturbation_norm = jnp.linalg.norm(jax.device_put(r))\n",
    "        best_label = true_label\n",
    "        best_corrupted_x = corrupted_x\n",
    "        # define the wagner and carlini attack problem\n",
    "        for label in range(self.n_classes): # to do : optimize this loop\n",
    "            if label != true_label:\n",
    "                def get_problem(w):\n",
    "                    w = self.project_to_bounds(w)\n",
    "                    target_label = label\n",
    "                    return self.get_objective(w, x, target_label, 1, k = 0, type = \"L2\")\n",
    "                # use adam optimizer to find minimum of the problem\n",
    "                optimizer = optax.adam(learning_rate=self.learning_rate)\n",
    "                state = optimizer.init(jax.device_put(w))\n",
    "                for i in range(self.max_iter):\n",
    "                    grad = jax.grad(get_problem)(jax.device_put(w))\n",
    "                    updates, state = optimizer.update(grad, state)\n",
    "                    w = optax.apply_updates(jax.device_put(w), updates)\n",
    "                    if self.get_label(1/2 * jnp.tanh(w) + 1/2) == label:\n",
    "                        break\n",
    "                # check if the attack was successful\n",
    "                new_label = self.get_label(1/2 * jnp.tanh(w) + 1/2)\n",
    "                if new_label != label:\n",
    "                    print(\"Warning: did not find a perturbation\")\n",
    "                    perturbation_norm = -1\n",
    "                else:\n",
    "                    perturbation_norm = jnp.linalg.norm(jax.device_put(w))\n",
    "                # Choose minimal perturbation\n",
    "                if perturbation_norm < max_perturbation_norm:\n",
    "                    max_perturbation_norm = perturbation_norm\n",
    "                    best_label = new_label\n",
    "                    best_corrupted_x = 1/2 * jnp.tanh(w) + 1/2\n",
    "\n",
    "        return best_corrupted_x, best_label, max_perturbation_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [07:22<1:06:23, 442.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [15:10<1:00:59, 457.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [22:13<51:32, 441.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [30:18<45:53, 458.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [37:31<37:27, 449.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [46:03<31:22, 470.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [54:23<24:00, 480.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:04:02<17:03, 511.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:14:19<09:04, 544.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n",
      "Warning: did not find a perturbation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:23:30<00:00, 501.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did not find a perturbation\n",
      "Average perturbation norm of Wagner & Carlini Attack model (on 10 successful samples): 17.2475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "all_xs, true_labels, epsilons, all_ys, K, test_key = init_data(test_key, n_samples=n_samples)\n",
    "\n",
    "corruption_model = WG_Attack(model)\n",
    "\n",
    "perturbation_norms_WG = get_average_performance(corruption_model, all_xs, epsilons, all_ys, K)\n",
    "perturbation_norms_successful_WG = perturbation_norms_WG[perturbation_norms_BFGS != -1]\n",
    "n_successful_WG = len(perturbation_norms_successful_WG)\n",
    "n_successful_WG\n",
    "print(f'Average perturbation norm of Wagner & Carlini Attack model (on {n_successful_BFGS} successful samples): {np.mean(perturbation_norms_successful_BFGS):>.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
