{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "import os\n",
    "from ml_collections import ConfigDict\n",
    "from pathlib import Path\n",
    "from utils import prepare_test_dataset\n",
    "from dataset_utils import get_dataset\n",
    "from jax import random\n",
    "from models.utils import sample_gaussian\n",
    "\n",
    "import models.ClassifierGFZ as ClassifierGFZ\n",
    "import models.ClassifierDFZ as ClassifierDFZ\n",
    "\n",
    "checkpoint_path = \"dfz-2-epochs-first-try-1\"\n",
    "path = os.path.join(Path.cwd(), Path(f\"checkpoints\"), Path(checkpoint_path))\n",
    "checkpoint = ocp.PyTreeCheckpointer().restore(path, item=None)\n",
    "\n",
    "config = ConfigDict(checkpoint[\"config\"])\n",
    "dataset_config = ConfigDict(checkpoint[\"dataset_config\"])\n",
    "\n",
    "if config.model_name == \"GFZ\":\n",
    "    classifier = ClassifierGFZ\n",
    "elif config.model_name == \"DFZ\":\n",
    "    classifier = ClassifierDFZ\n",
    "else:\n",
    "    raise NotImplementedError(config.model_name)\n",
    "\n",
    "_, test_ds = get_dataset(config.dataset)\n",
    "test_images, test_labels = prepare_test_dataset(\n",
    "    test_ds, dataset_config\n",
    "    )\n",
    "\n",
    "trained_params = checkpoint[\"params\"]\n",
    "\n",
    "log_likelyhood_fn = classifier.log_likelyhood_A\n",
    "\n",
    "test_key = random.PRNGKey(config.seed)\n",
    "\n",
    "test_key, model, _ = classifier.create_and_init(\n",
    "    test_key, config, dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax import jacrev\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def init_data(test_key, n_samples=10):\n",
    "    idx = np.random.choice(range(len(test_images)), n_samples, replace=False)\n",
    "\n",
    "    all_xs = test_images[idx]\n",
    "    true_ys = test_labels[idx]\n",
    "    true_labels = np.argmax(true_ys, axis=1)\n",
    "\n",
    "    K = model.K\n",
    "    batch_size = n_samples\n",
    "    test_key, epsilons = sample_gaussian(test_key, (batch_size, model.n_classes * K, model.d_latent))\n",
    "    epsilons = epsilons[:n_samples*model.n_classes]\n",
    "    all_ys = nn.one_hot(jnp.repeat(jnp.arange(model.n_classes), K), model.n_classes, dtype=jnp.float32)\n",
    "    \n",
    "    return all_xs, true_labels, epsilons, all_ys, K, test_key\n",
    "\n",
    "def get_model_output(x, epsilon, y, K):\n",
    "    z, logit_q_z_xy, logit_p_x_z, logit_p_y_xz = jax.vmap(\n",
    "            partial(model.apply, {'params': trained_params}, train=False),\n",
    "            in_axes=(None, 0, 0)\n",
    "        )(x, y, epsilon)\n",
    "\n",
    "    ll = log_likelyhood_fn(\n",
    "            z, logit_q_z_xy, logit_p_x_z, logit_p_y_xz\n",
    "        ).reshape(model.n_classes, K)\n",
    "    ll = logsumexp(ll, axis=1) - np.log(K)\n",
    "    return ll\n",
    "\n",
    "def get_model_jacobian(x, epsilon, y, K):\n",
    "    return jacrev(get_model_output, argnums=0)(x, epsilon, y, K)\n",
    "\n",
    "def map_label_to_name(y):\n",
    "    labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "              \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "    return labels[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeroth order optimization attack\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class ZOO_Attack():\n",
    "    def __init__(self, model, max_iter=10, learning_rate=0.1, c=1, p=2):\n",
    "        self.model = model\n",
    "        self.n_classes = model.n_classes\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.c = c\n",
    "        assert p > 1 \n",
    "        self.p = p\n",
    "        if self.p == np.inf:\n",
    "            self.q = 1\n",
    "        else:\n",
    "            self.q = self.p / (self.p - 1)\n",
    "\n",
    "    def qnorm(self, x):\n",
    "        return jnp.linalg.norm(x.flatten(), self.q)\n",
    "\n",
    "    def get_label(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jnp.argmax(val)\n",
    "\n",
    "    def get_outputs(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jnp.softmax(val)\n",
    "\n",
    "    def get_gradients(self, x, k, epsilon=1e-5):\n",
    "        # Estimate gradients using finite differences\n",
    "        e_k = jnp.ones_like(x)\n",
    "        e_k[k] = epsilon\n",
    "        perturbed_x_plus = x + e_k\n",
    "        perturbed_x_minus = x - e_k\n",
    "\n",
    "        output_plus = self.get_outputs(perturbed_x_plus)\n",
    "        output_minus = self.get_outputs(perturbed_x_minus)\n",
    "\n",
    "        gradient = (output_plus - output_minus) / (2 * epsilon)\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    def project_to_bounds(self, x):\n",
    "        bounds_min = jnp.zeros_like(x)\n",
    "        bounds_max = jnp.ones_like(x)\n",
    "        return jnp.clip(x, bounds_min, bounds_max)\n",
    "    \n",
    "    def f(self, x, target_label, k = 0):\n",
    "        x = self.project_to_bounds(x)\n",
    "        val = self.get_likelihoods(x)\n",
    "        max_logit = jnp.max(val[jnp.arange(self.n_classes) != target_label])\n",
    "        logit_diff = jnp.maximum(max_logit - val[target_label], - k)\n",
    "        return logit_diff\n",
    "    \n",
    "    def get_objective(self, w, x, target_label, k = 0):\n",
    "        norm = self.qnorm(w)\n",
    "        penalty = self.c * self.f(x + w, target_label, k = k)\n",
    "        return norm + penalty\n",
    "    \n",
    "    def get_obj_grad(self, w, x, target_label):\n",
    "        # Compute gradient of the objective function\n",
    "        corrupted_x = x + w\n",
    "        norm_grad = (2) * (corrupted_x - x)\n",
    "\n",
    "        val = self.get_likelihoods(corrupted_x)\n",
    "        grad_model = self.get_gradients(corrupted_x)\n",
    "        max_label = jnp.argmax(val[jnp.arange(self.n_classes) != target_label])\n",
    "        max_logit = val[max_label]\n",
    "        logit_diff = max_logit - val[target_label]\n",
    "        if logit_diff <= 0:\n",
    "            penalty_grad = 0\n",
    "        else:\n",
    "            penalty_grad = grad_model[max_label] - grad_model[target_label]\n",
    "        \n",
    "        return norm_grad + self.c * penalty_grad\n",
    "    \n",
    "    def project_to_bounds(self, x):\n",
    "        bounds_min = jnp.zeros_like(x)\n",
    "        bounds_max = jnp.ones_like(x)\n",
    "        return jnp.clip(x, bounds_min, bounds_max)\n",
    "\n",
    "    def get_perturbation(self, x, epsilon, all_ys, K):\n",
    "        self.y = all_ys\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "        true_label = self.get_label(x)\n",
    "        max_perturbation_norm = -1\n",
    "        best_label = true_label\n",
    "        best_corrupted_x = jnp.zeros_like(x)\n",
    "        # for label in range(self.n_classes): # to do : optimize this loop\n",
    "        for label in range(self.n_classes): # to do : optimize this loop\n",
    "            if label != true_label:\n",
    "                w = jnp.zeros_like(x)\n",
    "                # use gradient descent to find minimum of the problem\n",
    "                for i in tqdm(range(self.max_iter)):\n",
    "                    grad = self.get_obj_grad(w, x, label)\n",
    "                    w = w - self.learning_rate * grad\n",
    "                    corrupted_x = x + w\n",
    "                    corrupted_x = self.project_to_bounds(corrupted_x)\n",
    "                    \n",
    "                # check if the attack was successful\n",
    "                new_label = self.get_label(corrupted_x)\n",
    "                if new_label != label:\n",
    "                    print(\"Warning: did not find a perturbation for this label\")\n",
    "                    perturbation_norm = -1\n",
    "                else:\n",
    "                    perturbation_norm = np.linalg.norm(corrupted_x - x)/np.linalg.norm(x)\n",
    "                    print(\"Found a perturbation for label\", label, \"with norm\", perturbation_norm)\n",
    "                    print(corrupted_x)\n",
    "\n",
    "                # Choose minimal perturbation\n",
    "                if max_perturbation_norm == -1 and perturbation_norm != -1:\n",
    "                    max_perturbation_norm = perturbation_norm\n",
    "                    best_label = new_label\n",
    "                    best_corrupted_x = corrupted_x\n",
    "                else : \n",
    "                    if perturbation_norm != -1 and perturbation_norm < max_perturbation_norm:\n",
    "                        max_perturbation_norm = perturbation_norm\n",
    "                        best_label = new_label\n",
    "                        best_corrupted_x = corrupted_x\n",
    "\n",
    "        return best_corrupted_x, best_label, max_perturbation_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeroth order optimization attack\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class untargeted_ZOO_Attack():\n",
    "    def __init__(self, model, max_iter=10, step_size=0.1, c=1, p=2, batch_size = 32, eps =1e-8, b1 = 0.9, b2 = 0.999):\n",
    "        self.model = model\n",
    "        self.n_classes = model.n_classes\n",
    "        self.max_iter = max_iter\n",
    "        self.step_size = step_size\n",
    "        self.batch_size = batch_size\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.eps = eps\n",
    "        self.c = c\n",
    "        assert p > 1 \n",
    "        self.p = p\n",
    "        if self.p == np.inf:\n",
    "            self.q = 1\n",
    "        else:\n",
    "            self.q = self.p / (self.p - 1)\n",
    "\n",
    "    def qnorm(self, x):\n",
    "        return jnp.linalg.norm(x.flatten(), self.q)\n",
    "\n",
    "    def get_label(self, x):\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jnp.argmax(val)\n",
    "\n",
    "    def get_outputs(self, x):\n",
    "        x = np.reshape(x, (28, 28, -1))\n",
    "        val = get_model_output(x, self.epsilon, self.y, self.K)\n",
    "        return jax.nn.softmax(val)\n",
    "    \n",
    "    def project_to_bounds(self, x):\n",
    "        bounds_min = np.zeros_like(x)\n",
    "        bounds_max = np.ones_like(x)\n",
    "        return np.clip(x, bounds_min, bounds_max)\n",
    "    \n",
    "    def f(self, x):\n",
    "        val = np.log(self.get_outputs(x))\n",
    "        max_label = jnp.argmax(val[jnp.arange(self.n_classes) != self.true_label])\n",
    "        max_logit = val[max_label]\n",
    "        logit_diff = np.maximum(val[self.true_label] - max_logit, 0)\n",
    "        return logit_diff\n",
    "    \n",
    "    def get_objective(self, corrupted_x, x):\n",
    "        norm = self.qnorm(corrupted_x - x)\n",
    "        penalty = self.c * self.f(corrupted_x)\n",
    "        return norm + penalty\n",
    "    \n",
    "    def get_gradients(self, corrupted_x, x, k, epsilon=1e-5):\n",
    "        # Estimate gradients using finite differences\n",
    "        e_k = np.ones(x.shape[0])\n",
    "        e_k[k] = epsilon\n",
    "        perturbed_x_plus = corrupted_x + e_k\n",
    "        perturbed_x_minus = corrupted_x - e_k\n",
    "        output_plus = self.get_objective(perturbed_x_plus, x)\n",
    "        output_minus = self.get_objective(perturbed_x_minus, x)\n",
    "        gradient = (output_plus - output_minus) / (2 * epsilon)\n",
    "        return gradient\n",
    "    \n",
    "    def get_batch_gradients(self, corrupted_x, x, batch_indices, epsilon=1e-5):\n",
    "        gradients = np.zeros((len(batch_indices)))\n",
    "        for i, k in enumerate(batch_indices):\n",
    "            # Estimate gradients using finite differences\n",
    "            e_k = np.zeros(x.shape[0])\n",
    "            e_k[k] = epsilon\n",
    "            perturbed_x_plus = corrupted_x + e_k\n",
    "            perturbed_x_minus = corrupted_x - e_k\n",
    "            output_plus = self.get_objective(perturbed_x_plus, x)\n",
    "            output_minus = self.get_objective(perturbed_x_minus, x)\n",
    "            gradients[i] = (output_plus - output_minus) / (2 * epsilon)\n",
    "        return gradients\n",
    "\n",
    "    \n",
    "    def project_to_bounds(self, x):\n",
    "        bounds_min = np.zeros_like(x)\n",
    "        bounds_max = np.ones_like(x)\n",
    "        return np.clip(x, bounds_min, bounds_max)\n",
    "\n",
    "    def get_perturbation(self, x, epsilon, all_ys, K):\n",
    "        self.y = all_ys\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "        self.true_label = self.get_label(x)\n",
    "        shape = x.shape\n",
    "        x = x.flatten()\n",
    "        corrupted_x = x.copy()\n",
    "        i = 0\n",
    "        M = np.zeros_like(x)\n",
    "        v = np.zeros_like(x)\n",
    "        T = np.zeros_like(x)\n",
    "        # use ADAM to find minimum of the problem\n",
    "        while i < self.max_iter:\n",
    "            k = np.random.choice(x.shape[0])\n",
    "            grad = self.get_gradients(corrupted_x, x, k)\n",
    "            T[k] += 1\n",
    "            M[k] = self.b1 * M[k] + (1 - self.b1) * grad\n",
    "            v[k] = self.b2 * v[k] + (1 - self.b2) * grad**2\n",
    "            M_ = M[k] / (1 - np.power(self.b1, T[k]))\n",
    "            v_ = v[k] / (1 - np.power(self.b2, T[k])) \n",
    "            eta = - self.step_size / (np.sqrt(v_) + self.eps) * M_\n",
    "            corrupted_x = corrupted_x.at[k].set(corrupted_x[k] + eta)\n",
    "            corrupted_x = self.project_to_bounds(corrupted_x)\n",
    "            i += 1\n",
    "        # check if the attack was successful\n",
    "        corrupted_x = np.reshape(corrupted_x, shape)\n",
    "        new_label = self.get_label(corrupted_x)\n",
    "        if new_label == self.true_label:\n",
    "            print(\"Warning: did not find a perturbation\")\n",
    "            perturbation_norm = -1\n",
    "        else:\n",
    "            perturbation_norm = np.linalg.norm(corrupted_x - x)/np.linalg.norm(x)\n",
    "\n",
    "        return corrupted_x, new_label, perturbation_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_performance(corruption_model, all_xs, epsilons, all_ys, K):\n",
    "    perturbation_norms = []\n",
    "    n_samples = len(all_xs)\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        x = all_xs[i]\n",
    "        epsilon = epsilons[i]\n",
    "        _, _, perturbation_norm = corruption_model.get_perturbation(x, epsilon, all_ys, K)\n",
    "        perturbation_norms.append(perturbation_norm)\n",
    "    return np.array(perturbation_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1\n",
    "all_xs, true_labels, epsilons, all_ys, K, test_key = init_data(test_key, n_samples=n_samples)\n",
    "\n",
    "corruption_model = untargeted_ZOO_Attack(model, batch_size=5, step_size = 1, max_iter=100, c=1, p=2)\n",
    "\n",
    "perturbation_norms_ZOO = get_average_performance(corruption_model, all_xs, epsilons, all_ys, K)\n",
    "perturbation_norms_successful_ZOO = perturbation_norms_ZOO[perturbation_norms_ZOO != -1]\n",
    "n_successful_ZOO = len(perturbation_norms_successful_ZOO)\n",
    "n_successful_ZOO\n",
    "print(f'Average perturbation norm of ZOO Attack model (on {n_successful_ZOO} successful samples): {np.mean(perturbation_norms_successful_ZOO):>.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(range(n_samples))\n",
    "x = all_xs[i]\n",
    "true_label = true_labels[i]\n",
    "test_key, epsilons = sample_gaussian(test_key, (1, model.n_classes * K, model.d_latent))\n",
    "epsilon = epsilons[0]\n",
    "\n",
    "corruption_model = untargeted_ZOO_Attack(model, max_iter=100, step_size=0.01, c = 1, p=2)\n",
    "corrupted_x, new_label, perturbation_norm = corruption_model.get_perturbation(x, epsilon, all_ys, K)\n",
    "w = corrupted_x - x\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axs[0].imshow(x.reshape(28, 28), cmap=\"gray\")\n",
    "axs[0].set_title(f\"Original image (label = '{map_label_to_name(true_label)}')\")\n",
    "\n",
    "axs[1].imshow(w.reshape(28, 28), cmap=\"gray\")\n",
    "axs[1].set_title(\"Perturbation\")\n",
    "\n",
    "axs[2].imshow(corrupted_x.reshape(28, 28), cmap=\"gray\")\n",
    "axs[2].set_title(f\"ZOO perturbated image (label = '{map_label_to_name(new_label)}')\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
