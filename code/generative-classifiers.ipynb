{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, jit, vmap\n",
    "from jax.nn import one_hot\n",
    "from jax.nn.initializers import glorot_uniform\n",
    "from jax.tree_util import tree_map\n",
    "from functools import partial # needed to make arguments static in jit compiled code\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "from optax import adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-deepmind/dm-haiku/issues/18#issuecomment-981814403\n",
    "MODELS_PATH = \"../model/\"\n",
    "def save_models(model, path: str):\n",
    "    with open(os.path.join(MODELS_PATH, path), \"wb\") as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpuDevice(id=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because Apple sucks\n",
    "jax.default_device = jax.devices(\"cpu\")[0]\n",
    "jax.default_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store all the parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1], dtype=uint32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['seed'] = 1\n",
    "key = random.PRNGKey(args['seed'])\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### FashionMNIST: a realistic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['batch_size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=jnp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images (60000, 784) train_labels (60000,)\n",
      "test_images (10000, 784) test_labels (10000,)\n",
      "n_classes 10 classes [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist_train_ds = FashionMNIST(\n",
    "    './data/', \n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=FlattenAndCast(),\n",
    ")\n",
    "n_train_images = len(fashion_mnist_train_ds.data)\n",
    "classes = np.unique(fashion_mnist_train_ds.targets)\n",
    "n_classes = len(classes)\n",
    "\n",
    "fashion_mnist_test_ds = FashionMNIST(\n",
    "    './data/', \n",
    "    download=True, \n",
    "    train=False,\n",
    ")\n",
    "n_test_images = len(fashion_mnist_test_ds.data)\n",
    "\n",
    "# Dataloader used for the training\n",
    "fashion_mnist_train_dl = NumpyLoader(\n",
    "    fashion_mnist_train_ds, \n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = jnp.array(fashion_mnist_train_ds.data).reshape(n_train_images, -1)\n",
    "train_labels = jnp.array(fashion_mnist_train_ds.targets)\n",
    "print(\"train_images\", train_images.shape, \"train_labels\", train_labels.shape)\n",
    "\n",
    "# Get full test dataset\n",
    "test_images = jnp.array(fashion_mnist_test_ds.data.numpy().reshape(n_test_images, -1))\n",
    "test_labels = jnp.array(fashion_mnist_test_ds.targets)\n",
    "print(\"test_images\", test_images.shape, \"test_labels\", test_labels.shape)\n",
    "\n",
    "print(\"n_classes\", n_classes, \"classes\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model where $\\theta$ and $\\phi$ are learnable parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take the GFZ probabilistic graphical model for a first implementation: $p(x,z,y)=p(z)p(y|z)p(x|z,y)$.\n",
    "\n",
    "`\n",
    "q(z|x,y) is the same across all VAE-based classifiers. It starts with a 3-layer\n",
    "convolutional neural network with 5 ×5 filters and 64 channels, with a max-pooling operation after each convolution. Then,\n",
    "the convolutional network is followed by a MLP with 2 hidden layers, each with 500 units, to produce the mean and variance\n",
    "parameters of q. The label y is injected into the MLP at the first hidden layer, as a one hot encoding (i.e. for MNIST, the\n",
    "first hidden layer has 500+10 units). The latent dimension is dim(z) = 64.\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    n_classes = 10\n",
    "    d_epsilon = 64\n",
    "    n_convolutions = 3\n",
    "    n_channels = 64\n",
    "    kernel_size = (5, 5)\n",
    "    strides = (2, 2)\n",
    "    d_hidden = 500\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, y, epsilon): # X: (height, width), y: (n_classes,), epsilon: (d_epsilon,) -> (d_epsilon,), 0\n",
    "        for _ in range(self.n_convolutions):\n",
    "            X = nn.Conv(\n",
    "                features=self.n_channels, \n",
    "                kernel_size=self.kernel_size, \n",
    "                strides=self.strides, \n",
    "                kernel_init=glorot_uniform(),\n",
    "            )(X)\n",
    "            X = nn.relu(X)\n",
    "\n",
    "        X_flatten = X.reshape(-1)\n",
    "        X_flatten = nn.Dense(\n",
    "            features=self.d_hidden, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(X_flatten)\n",
    "        X_flatten = nn.relu(X_flatten)\n",
    "        \n",
    "        output = jnp.concatenate((X_flatten, y), axis=0)\n",
    "        output = nn.Dense(\n",
    "            features=self.d_hidden + self.n_classes, \n",
    "            use_bias=False,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(output)\n",
    "        output = nn.relu(output)\n",
    "\n",
    "        output = nn.Dense(\n",
    "            features=2 * self.d_epsilon, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(output)\n",
    "        # end of model\n",
    "\n",
    "        mu, log_sigma2 = jnp.split(output, 2)\n",
    "        # this should be correct: to change by recomputing\n",
    "        logdet = jnp.sum(log_sigma2)\n",
    "        sigma = jnp.sqrt(jax.lax.pow(10.0, logdet))\n",
    "        z = mu + sigma * epsilon\n",
    "        logits =  - (self.d_epsilon * logdet + jnp.dot(epsilon, epsilon)) / 2\n",
    "        return z, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "For p(y|z) we use a MLP with 1 hidden layer composed of 500 units. For p(x|y,z) we used an MLP with 2\n",
    "hidden layers, each with 500 units, and 4 ×4 ×64 dimension output, followed by a 3-layer deconvolutional network\n",
    "with 5 ×5 kernel size, stride 2 and [64, 64, 1] channels.\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement forward pass\n",
    "class Log_p_y_z(nn.Module):\n",
    "    d_hidden = 500\n",
    "    n_classes = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, y, z): # y: (n_classes,), z: (d_epsilon,) -> (n_classes,)\n",
    "        logits = nn.Dense(\n",
    "            features=self.d_hidden, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(z)\n",
    "        logits = nn.relu(logits)\n",
    "        logits = nn.Dense(\n",
    "            features=self.n_classes, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(logits)\n",
    "        \n",
    "        return logits # use cross entropy with logits\n",
    "\n",
    "#TODO: implement\n",
    "class Log_p_x_yz(nn.Module):\n",
    "    d_hidden = 500\n",
    "    d_epsilon = 64\n",
    "    n_classes = 10\n",
    "    n_channels = 64\n",
    "    input_kernel = (4, 4)\n",
    "    kernel_size = (5, 5)\n",
    "    strides = (2, 2)\n",
    "    \n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, y, z): # X: (height, width), y: (n_classes,), z: (d_epsilon,) -> 0\n",
    "        inputs = jnp.concatenate([y, z], 0)\n",
    "        inputs = nn.Dense(\n",
    "            features=self.d_hidden, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(inputs)\n",
    "        inputs = nn.relu(inputs)\n",
    "        inputs = nn.Dense(\n",
    "            features=np.prod(self.input_kernel) * self.n_channels, \n",
    "            use_bias=True,\n",
    "            kernel_init=glorot_uniform(), \n",
    "        )(inputs)\n",
    "        inputs = nn.relu(inputs)\n",
    "        inputs = inputs.reshape(self.input_kernel + (self.n_channels,))\n",
    "\n",
    "        inputs = nn.ConvTranspose(\n",
    "            features=2*self.d_epsilon,\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=self.strides,\n",
    "            padding=(2, 2),\n",
    "            kernel_init=glorot_uniform(),\n",
    "        )(inputs)\n",
    "        inputs = nn.relu(inputs)\n",
    "\n",
    "        inputs = nn.ConvTranspose(\n",
    "            features=2*self.d_epsilon,\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=self.strides,\n",
    "            padding=((2, 3), (2, 3)),\n",
    "            kernel_init=glorot_uniform(),\n",
    "        )(inputs)\n",
    "        inputs = nn.relu(inputs)\n",
    "\n",
    "        inputs = nn.ConvTranspose(\n",
    "            features=2,\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=self.strides,\n",
    "            padding=((2, 3), (2, 3)),\n",
    "            kernel_init=glorot_uniform(),\n",
    "        )(inputs)\n",
    "        inputs = nn.sigmoid(inputs) #this sounds wrong! logsigma2 lives in R and not (-1, 1)\n",
    "\n",
    "        mu, log_sigma2 = jnp.split(inputs, 2, axis=2)\n",
    "\n",
    "        # this should be correct: to change by recomputing\n",
    "        n = X.ndim\n",
    "        sigma2 = jax.lax.pow(10.0, log_sigma2)\n",
    "        delta = (X - mu).reshape(-1)\n",
    "        logits =  - (n * log_sigma2 + jnp.dot(delta, delta) / sigma2) / 2\n",
    "        return logits\n",
    "    \n",
    "# (GFZ) graph model\n",
    "class Log_p_xzy(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, y, z): # X: (height, width), y: (n_classes,), z: (d_epsilon,) -> 0\n",
    "        d_epsilon = z.shape[0]\n",
    "        log_prior = -(d_epsilon + jnp.dot(z, z))/2 # assuming it's standard Gaussian\n",
    "        return log_prior + Log_p_y_z()(y, z) + Log_p_x_yz()(X, y, z)\n",
    "    \n",
    "class ModelGFZ(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, y, epsilon): # X: (height, width), y: (n_classes,), epsilon: (d_epsilon,) -> 1, 1\n",
    "        z, logits_q = Encoder()(X, y, epsilon)\n",
    "        logits_p = Log_p_xzy()(X, y, z)\n",
    "        return logits_p, logits_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) ()\n"
     ]
    }
   ],
   "source": [
    "X = jnp.ones((28, 28, 1))\n",
    "y = jnp.ones(10)\n",
    "epsilon = jnp.ones(64)\n",
    "\n",
    "model = ModelGFZ()\n",
    "params = model.init(key, X, y, epsilon)\n",
    "log_p, log_q = model.apply(params, X, y, epsilon)\n",
    "print(log_p.shape, log_q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the loss functions defined in \"Auto Encoding Varational Bayes\" to approximate the true ELBO $\\cal L$:\n",
    "\n",
    "$$\n",
    "\\widetilde{\\cal L}^{A}(\\theta,\\phi;{\\bf x}^{(i)}, {\\bf y}^{(i)})=\\frac{1}{L}\\sum_{l=1}^{L}\\log p_{\\theta}({\\bf x}^{(i)},{\\bf z}^{(i,l)},{\\bf y}^{(i)})-\\log q_{\\phi}({\\bf z}^{(i,l)}|{\\bf x}^{(i)}, {\\bf y}^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\widetilde{\\cal L}^{B}(\\theta,\\phi;{\\bf x}^{(i)}, {\\bf y}^{(i)})=-D_{K L}(q_{\\phi}({\\bf z}|{\\bf x}^{(i)}, {\\bf y}^{(i)})||p_{\\theta}({\\bf z}))+\\frac{1}{L}\\sum_{l=1}^{L} \\log p_{\\theta}({\\bf x}^{(i)}|{\\bf z}^{(i,l)}, {\\bf y}^{(i)})\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\widetilde{\\cal L}^{M}(\\theta,\\phi;{\\bf X}^{M}, {\\bf y}^{M}, {\\bf \\epsilon})=\\frac{N}{M}\\sum_{i=1}^{M}=\\widetilde{\\cal L}^{A\\text{ or }B}(\\theta,\\phi;{\\bf X}^{M}_i, {\\bf y}^{M}_i)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function \\widetilde{\\cal L}^{A} for a pair (X, y) and an array of epsilon\n",
    "def loss_A_single(X: jnp.ndarray, y: jnp.ndarray, epsilon: jnp.ndarray) -> float:\n",
    "    z, logits_q = log_q_z_xy(epsilon, X, y)\n",
    "    logits_p = log_p_xzy(X, z, y)\n",
    "    return jnp.mean(logits_p - logits_q, axis=-1)\n",
    "\n",
    "loss_A_batch = vmap(loss_A_single, in_axes=(0, 0, 0), out_axes=0)\n",
    "def loss_M_A(X_batch: jnp.ndarray, y_batch: jnp.ndarray, epsilon: jnp.ndarray):\n",
    "    return jnp.sum(loss_A_batch(X_batch, y_batch, epsilon))\n",
    "\n",
    "def loss_B(theta: jnp.ndarray, phi: jnp.ndarray, x: jnp.ndarray, y: jnp.ndarray, eps: jnp.ndarray) -> float:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(apply_fn, X_batch, y_batch, epsilon, opt_state, params, state):\n",
    "  def batch_loss(params):\n",
    "    def loss_fn(X, y, epsilon):\n",
    "      log_p, log_q, updated_state = apply_fn(\n",
    "        {'params': params, **state},\n",
    "        X, y, epsilon, mutable=list(state.keys())\n",
    "      )\n",
    "      return log_p - log_q, updated_state\n",
    "\n",
    "    loss, updated_state = jax.vmap(\n",
    "      loss_fn, \n",
    "      in_axes=(0, 0, 0), \n",
    "      out_axes=(0, None), # Do not vmap `updated_state`.\n",
    "      axis_name='batch' # Name batch dim\n",
    "    )(X_batch, y_batch, epsilon)  # vmap only `X`, `y`, but not `state`.\n",
    "    return jnp.mean(loss), updated_state\n",
    "\n",
    "  (loss, updated_state), grads = jax.value_and_grad(\n",
    "    batch_loss, has_aux=True\n",
    "  )(params)\n",
    "\n",
    "  updates, opt_state = tx.update(grads, opt_state)  # Defined below.\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return opt_state, params, updated_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise distribution $p$ that parameterises latent variable $z$, i.e. $z = g_{\\theta}(\\epsilon, x, y)$ where $\\epsilon\\sim p(\\epsilon)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_p(key: jnp.ndarray, epsilon_shape: tuple) -> np.ndarray:\n",
    "    key, sample_key = random.split(key)\n",
    "    epsilon = random.normal(sample_key, epsilon_shape)\n",
    "    return key, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnames=['learning_rate'])\n",
    "def update_sgd(grad_theta: np.ndarray, grad_phi: np.ndarray, learning_rate: float):\n",
    "    return -learning_rate * grad_theta, -learning_rate * grad_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['num_epochs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'tuple' is an illegal expression for augmented assignment (4129801499.py, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[44], line 54\u001b[0;36m\u001b[0m\n\u001b[0;31m    theta, phi += update(grad_theta, grad_phi, learning_rate=learning_rate)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'tuple' is an illegal expression for augmented assignment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for the validation loss logging\n",
    "n_log_validation = 100\n",
    "\n",
    "# definition of remaining parameters\n",
    "dim_batch = 100\n",
    "d_epsilon = 64\n",
    "\n",
    "# gradient descent method and associated parameters: use optimisation algo implemented in Optax if needed\n",
    "learning_rate = 0.01\n",
    "update = update_sgd\n",
    "\n",
    "# initial parameter values\n",
    "theta_0 = jnp.ones(10)\n",
    "phi_0 = jnp.ones(10)\n",
    "\n",
    "# optimised parameters\n",
    "theta = jnp.copy(theta_0)\n",
    "phi = jnp.copy(phi_0)\n",
    "\n",
    "#sink for the loss values\n",
    "training_steps, validation_steps = [], []\n",
    "training_loss_values, validation_loss_values = [], []\n",
    "\n",
    "epsilon_shape = (n_batch, d_epsilon)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(args['num_epochs']):\n",
    "    for train_step, (X_batch, y_batch) in tqdm(enumerate(fashion_mnist_train_dl), total=n_train_images):\n",
    "        # one-hot encoding\n",
    "        y_batch_one_hot = one_hot(y_batch, n_classes)\n",
    "        \n",
    "        # sample from noise distribution\n",
    "        key, epsilon = sample_p(key, epsilon_shape)\n",
    "\n",
    "        #compute the loss value and grad w.r.t. phi/theta\n",
    "        grad_theta = jax.grad(loss, argnums=0)(theta, phi, X_M, y_M, eps)\n",
    "        loss_value, grad_phi = jax.value_and_grad(loss, argnums=1)(theta, phi, X_M, y_M, eps)\n",
    "        \n",
    "        # log the training loss: here it's just the batch loss, need to be fixed.\n",
    "        training_loss_values.append(loss_value)\n",
    "        training_steps.append(train_step)\n",
    "\n",
    "        # log the validation loss for some steps\n",
    "        if (train_step+1) % n_log_validation == 0:\n",
    "            validation_loss_value = compute_validation_loss(theta, phi)\n",
    "            validation_loss_values.append(validation_loss_value)\n",
    "            validation_steps.append(train_step)\n",
    "\n",
    "        # update the parameters according to chosen policy\n",
    "        theta, phi += update(grad_theta, grad_phi, learning_rate=learning_rate)\n",
    "\n",
    "save_models({\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"N_trains\": N_trains,\n",
    "    \"M\": M,\n",
    "    \"theta\": theta,\n",
    "    \"phi\": phi,\n",
    "}, f\"models/test_{datetime.today().strftime('%Y-%m-%d_%H:%M:%S')}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting training/test loss plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_steps, training_loss_values, label=[\"training\"])\n",
    "ax.plot(validation_steps, validation_loss_values, label=[\"validation\"])\n",
    "ax.set_xlabel(\"iteration\")\n",
    "fig.suptitle(\"Loss values during training\")\n",
    "_ = ax.set_ylabel(\"$\\cal L$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
