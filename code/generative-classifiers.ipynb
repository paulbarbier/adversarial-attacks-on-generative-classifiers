{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, jit\n",
    "from jax.tree_util import tree_map\n",
    "from functools import partial # needed to make arguments static in jit compiled code\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "from optax import adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-deepmind/dm-haiku/issues/18#issuecomment-981814403\n",
    "MODELS_PATH = \"../model/\"\n",
    "def save_models(model, path: str):\n",
    "    with open(os.path.join(MODELS_PATH, path), \"wb\") as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because Apple sucks\n",
    "jax.default_device = jax.devices(\"cpu\")[0]\n",
    "jax.default_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store all the parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['seed'] = 1\n",
    "key = random.PRNGKey(args['seed'])\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### FashionMNIST: a realistic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['batch_size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=jnp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_train_ds = FashionMNIST(\n",
    "    './data/', \n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=FlattenAndCast(),\n",
    ")\n",
    "n_train_images, n_train_classes = len(fashion_mnist_train_ds.data), len(fashion_mnist_train_ds.targets)\n",
    "\n",
    "fashion_mnist_test_ds = FashionMNIST(\n",
    "    './data/', \n",
    "    download=True, \n",
    "    train=False,\n",
    ")\n",
    "n_test_images, n_test_classes = len(fashion_mnist_test_ds.data), len(fashion_mnist_test_ds.targets)\n",
    "\n",
    "# Dataloader used for the training\n",
    "fashion_mnist_train_dl = NumpyLoader(\n",
    "    fashion_mnist_train_ds, \n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = jnp.array(fashion_mnist_train_ds.data).reshape(n_train_images, -1)\n",
    "train_labels = jnp.array(fashion_mnist_train_ds.targets)\n",
    "print(\"train_images\", train_images.shape, \"train_labels\", train_labels.shape)\n",
    "\n",
    "# Get full test dataset\n",
    "test_images = jnp.array(fashion_mnist_test_ds.data.numpy().reshape(n_test_images, -1))\n",
    "test_labels = jnp.array(fashion_mnist_test_ds.targets)\n",
    "print(\"test_images\", test_images.shape, \"test_labels\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model where $\\theta$ and $\\phi$ are learnable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_theta_xzy(theta: jnp.ndarray, x: float, z: float, y:float ) -> float:\n",
    "    pass\n",
    "\n",
    "def q_phi_z_xy(phi: jnp.ndarray, z: float, x: float, y: float) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the loss functions defined in \"Auto Encoding Varational Bayes\" to approximate the true ELBO $\\cal L$:\n",
    "\n",
    "$$\n",
    "\\widetilde{\\cal L}^{A}(\\theta,\\phi;{\\bf x}^{(i)}, {\\bf y}^{(i)})=\\frac{1}{L}\\sum_{l=1}^{L}\\log p_{\\theta}({\\bf x}^{(i)},{\\bf z}^{(i,l)},{\\bf y}^{(i)})-\\log q_{\\phi}({\\bf z}^{(i,l)}|{\\bf x}^{(i)}, {\\bf y}^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\widetilde{\\cal L}^{B}(\\theta,\\phi;{\\bf x}^{(i)}, {\\bf y}^{(i)})=-D_{K L}(q_{\\phi}({\\bf z}|{\\bf x}^{(i)}, {\\bf y}^{(i)})||p_{\\theta}({\\bf z}))+\\frac{1}{L}\\sum_{l=1}^{L} \\log p_{\\theta}({\\bf x}^{(i)}|{\\bf z}^{(i,l)}, {\\bf y}^{(i)})\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\widetilde{\\cal L}^{M}(\\theta,\\phi;{\\bf X}^{M}, {\\bf y}^{M}, {\\bf \\epsilon})=\\frac{1}{M}\\sum_{i=1}^{M}=\\widetilde{\\cal L}^{A/B}(\\theta,\\phi;{\\bf X}^{M}_i, {\\bf y}^{M}_i)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_A(theta: jnp.ndarray, phi: jnp.ndarray, x: jnp.ndarray, y: jnp.ndarray, eps: jnp.ndarray) -> float:\n",
    "    return 0\n",
    "\n",
    "def loss_B(theta: jnp.ndarray, phi: jnp.ndarray, x: jnp.ndarray, y: jnp.ndarray, eps: jnp.ndarray) -> float:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise distribution $p$ that parameterises latent variable $z$, i.e. $z = g_{\\theta}(\\epsilon, x, y)$ where $\\epsilon\\sim p(\\epsilon)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_p(keys: np.ndarray) -> np.ndarray:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnames=['learning_rate'])\n",
    "def update_sgd(grad_theta: np.ndarray, grad_phi: np.ndarray, learning_rate: float):\n",
    "    return -learning_rate * grad_theta, -learning_rate * grad_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['num_epochs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the validation loss logging\n",
    "n_log_validation = 100\n",
    "\n",
    "# definition of remaining parameters\n",
    "M = 10\n",
    "N_trains = 10\n",
    "d_eps = 10\n",
    "\n",
    "# gradient descent method and associated parameters: use optimisation algo implemented in Optax if needed\n",
    "learning_rate = 0.01\n",
    "update = update_sgd\n",
    "\n",
    "# used loss function\n",
    "loss = loss_A\n",
    "\n",
    "# initial parameter values\n",
    "theta_0 = jnp.ones(10)\n",
    "phi_0 = jnp.ones(10)\n",
    "\n",
    "# optimised parameters\n",
    "theta = jnp.copy(theta_0)\n",
    "phi = jnp.copy(phi_0)\n",
    "\n",
    "#sink for the loss values\n",
    "training_steps, validation_steps = [], []\n",
    "training_loss_values, validation_loss_values = [], []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(args['num_epochs']):\n",
    "    for train_step, (X_batch, y_batch) in tqdm(enumerate(fashion_mnist_train_dl):\n",
    "        # sample from noise distribution\n",
    "        key, *sample_keys = random.split(key)\n",
    "        key, eps = sample_p(sample_key, epsilon_shape)\n",
    "\n",
    "        #compute the loss value and grad w.r.t. phi/theta\n",
    "        grad_theta = jax.grad(loss, argnums=0)(theta, phi, X_M, y_M, eps)\n",
    "        loss_value, grad_phi = jax.value_and_grad(loss, argnums=1)(theta, phi, X_M, y_M, eps)\n",
    "        \n",
    "        # log the training loss: here it's just the batch loss, need to be fixed.\n",
    "        training_loss_values.append(loss_value)\n",
    "        training_steps.append(train_step)\n",
    "\n",
    "        # log the validation loss for some steps\n",
    "        if (train_step+1) % n_log_validation == 0:\n",
    "            validation_loss_value = compute_validation_loss(theta, phi)\n",
    "            validation_loss_values.append(validation_loss_value)\n",
    "            validation_steps.append(train_step)\n",
    "\n",
    "        # update the parameters according to chosen policy\n",
    "        theta, phi += update(grad_theta, grad_phi, learning_rate=learning_rate)\n",
    "\n",
    "save_models({\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"N_trains\": N_trains,\n",
    "    \"M\": M,\n",
    "    \"theta\": theta,\n",
    "    \"phi\": phi,\n",
    "}, f\"models/test_{datetime.today().strftime('%Y-%m-%d_%H:%M:%S')}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting training/test loss plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_steps, training_loss_values, label=[\"training\"])\n",
    "ax.plot(validation_steps, validation_loss_values, label=[\"validation\"])\n",
    "ax.set_xlabel(\"iteration\")\n",
    "fig.suptitle(\"Loss values during training\")\n",
    "_ = ax.set_ylabel(\"$\\cal L$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
